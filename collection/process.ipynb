{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "from src.dbmongo import DbMongo, get_db\n",
    "from collections import defaultdict, Counter\n",
    "from loguru import logger\n",
    "import csv\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parse_config(\"config.json\")\n",
    "db = get_db(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update entitie's len\n",
    "db.update_entity_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual fixes\n",
    "db.update_entities_to(config[\"actually_people\"], \"PER\")\n",
    "db.update_entities_to(config[\"actually_orgs\"], \"ORG\")\n",
    "db.update_entities_to(config[\"actually_locs\"], \"LOC\")\n",
    "db.update_entities_to(config[\"actually_misc\"], \"MISC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data and generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 delete everything from the \"final\" collections\n",
    "db[\"final_entities\"].drop()\n",
    "db[\"final_news\"].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 find duplicates and update\n",
    "def update_dups():\n",
    "    # delete all \"duplicate_of\" before running again\n",
    "    db[\"entities\"].update({}, {\"$unset\": {\"duplicate_of\": True}} , multi=True)\n",
    "    db.update_duplicate_entities(label=\"PER\")\n",
    "    db.update_duplicate_entities(label=\"ORG\")\n",
    "    db.update_duplicate_entities(label=\"LOC\")\n",
    "    db.update_duplicate_entities(label=\"MISC\")\n",
    "#update_dups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_entities = list(set(config[\"ignore_people\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_entity(e):\n",
    "    if len(e[\"_id\"]) <= 1: return False\n",
    "    if e[\"_id\"][0:2] in {\"a-\", \"o-\", \"f-\"}: return False\n",
    "    if e[\"_id\"][0:3] in {\"dr-\", \"ao-\"}: return False\n",
    "    if e[\"_id\"][0:5] in {\"foto-\"}: return False\n",
    "    if e[\"_id\"][-4:] in {\"-dos\", \"-lhe\", \"-lho\", \"-lha\"}: return False\n",
    "    if e[\"_id\"][-3:] in {\"-de\", \"-da\", \"-do\", \"-dn\", \"-cm\", \"-jn\", \"-in\"}: return False\n",
    "    if any([forbidden in e[\"_id\"] for forbidden in [\"soundcloud\", \"itunes\", \"android\", \"http\", \"www\", \"youtube\", \"instagram\"]]): return False\n",
    "    if all([len(part)==1 for part in e[\"_id\"].split(\"-\")]): return False # prevenir a-b-c\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_entity_size_per_label(e):\n",
    "    label = e[\"label\"]\n",
    "    if label == \"PER\":  return \"-\" in e[\"_id\"] and e[\"len\"]>=10\n",
    "    if label == \"ORG\":  return e[\"len\"] >= max(50, 110 - 30 * len(e[\"_id\"].split(\"-\")))\n",
    "    if label == \"LOC\":  return e[\"len\"] >= 50\n",
    "    if label == \"MISC\": return e[\"len\"] >= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def must_include(e):\n",
    "    return e[\"_id\"] in config[\"must_include\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 iterate non-duplicate entities with a minmimum len according to label, previously: \"_id\": {\"$regex\": \".+-.+\"}\n",
    "for e in db[\"entities\"].find({\"duplicate_of\": {\"$exists\": False}, \"len\": {\"$gte\": 10}, \"_id\": {\"$not\": {\"$in\": ignore_entities}}}, no_cursor_timeout=True):\n",
    "    if not must_include(e) and (not valid_entity(e) or not valid_entity_size_per_label(e)): continue\n",
    "    e[\"search_text\"] = e[\"text\"]\n",
    "    db._upsert_one(\"final_entities\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all that are root of duplicate_of but are single worded like \"zapatero\"\n",
    "# if these are insignificant -> ignore\n",
    "for dup in db[\"entities\"].find({\"duplicate_of\": {\"$exists\": True, \"$regex\": \"^[^-]+$\", \"$not\": {\"$in\": ignore_entities}}}, {\"duplicate_of\": True}, no_cursor_timeout=True).limit(10):\n",
    "    print(dup[\"duplicate_of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate all duplicates of those entities and update their search terms\n",
    "for dup in db[\"entities\"].find({\"duplicate_of\": {\"$exists\": True}}, {\"duplicate_of\": True, \"news\": True, \"text\": True}, no_cursor_timeout=True):\n",
    "    root = db[\"final_entities\"].find_one({\"_id\": dup[\"duplicate_of\"]})\n",
    "    if not root: continue\n",
    "    # manually because mongo did not allow one-op-update\n",
    "    root[\"search_text\"]+=\"\\n%s\"%dup[\"text\"]\n",
    "    root[\"news\"] = list(set(root[\"news\"] + dup[\"news\"]))\n",
    "    root[\"len\"] = len(root[\"news\"])\n",
    "    db._upsert_one(\"final_entities\", root, upsert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate all valid and processed news -> insert into final_news without entities\n",
    "# SHOULD RUN rnd/fix_news_texts beforehand\n",
    "for n in db[\"news\"].find({\"processed\": True, \"processed_entities\": True, \"valid\": {\"$exists\": False}}, {\"title\":True,\"original\":True,\"text\":True,\"timestamp\":True,\"url\":True,\"image\":True, \"website\": True}, no_cursor_timeout=True):\n",
    "    n[\"entities\"] = {\"PER\": [], \"ORG\": [], \"LOC\": [], \"MISC\": []}\n",
    "    db._upsert_one(\"final_news\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate all final_entities and insert them into the news piece\n",
    "from pymongo import UpdateOne\n",
    "for fe in db[\"final_entities\"].find({}, no_cursor_timeout=True):\n",
    "    db[\"final_news\"].bulk_write([UpdateOne({'_id': n}, {\n",
    "        \"$addToSet\": {\"entities.%s\" % fe[\"label\"]: {\"$each\": [{\"_id\": fe[\"_id\"], \"text\": fe[\"text\"]}]}}\n",
    "    }, upsert=True) for n in fe[\"news\"]], ordered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_news = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any news_piece without entities of type PER and ORG\n",
    "filters = {\n",
    "    \"entities.PER\": {\"$size\": 0}, \n",
    "    \"entities.ORG\": {\"$size\": 0}, \n",
    "    # \"entities.LOC\": {\"$size\": 0}, \n",
    "    # \"entities.MISC\": {\"$size\": 0}\n",
    "}\n",
    "removed_news += [n[\"_id\"] for n in db['final_news'].find(filters)]\n",
    "db['final_news'].remove(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any news_piece without \"text\", this happens when entities are added to a non-existing news piece\n",
    "# probably from a bug in the collection process or due to having performed several operations on it\n",
    "filters = {\"text\": {\"$exists\": False}}\n",
    "removed_news += [n[\"_id\"] for n in db['final_news'].find(filters)]\n",
    "db['final_news'].remove(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_news = set(removed_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all the news mentioned in the final_entites are in final_news otherwise remove those news_ids from their `news` attributes\n",
    "final_entities = list(db[\"final_entities\"].find({}, no_cursor_timeout=True))\n",
    "pbar = tqdm(total=len(final_entities))\n",
    "for fe in final_entities:\n",
    "    original = fe[\"news\"]\n",
    "    actual = list(set(fe[\"news\"]) - removed_news)\n",
    "    if len(actual) < len(original): # if some of the news have been deleted above\n",
    "        fe[\"news\"] = actual\n",
    "        db._upsert_one(\"final_entities\", fe)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Neo4j code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neo4j.com/blog/bulk-data-import-neo4j-3-0/\n",
    "# https://neo4j.com/blog/cypher-write-fast-furious/\n",
    "# https://medium.com/neo4j/5-tips-tricks-for-fast-batched-updates-of-graph-structures-with-neo4j-and-cypher-73c7f693c8cc\n",
    "# https://neo4j.com/developer/guide-import-csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import command\n",
    "```\n",
    "CREATE INDEX ON :PER(_id);\n",
    "CREATE INDEX ON :ORG(_id);\n",
    "CREATE INDEX ON :LOC(_id);\n",
    "CREATE INDEX ON :MISC(_id);\n",
    "CREATE INDEX ON :NEWS(_id);\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "`MATCH (n) DETACH DELETE n`\n",
    "\n",
    "`:schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate every entitiy\n",
    "def generate_csv(csv_name, label):\n",
    "    with open(\"../neo4j/import/%s.csv\" % csv_name, \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "        # fields = ['_id', 'label', 'text', 'search_text', 'len', 'news']\n",
    "        fields = ['_id', 'text', \"news\"]\n",
    "        writer = csv.DictWriter(out, fieldnames=fields, delimiter=\",\")\n",
    "        writer.writeheader()\n",
    "        for fe in db[\"final_entities\"].find({\"label\": label}, {k: True for k in fields}, no_cursor_timeout=True):\n",
    "            fe[\"news\"] = \",\".join(fe[\"news\"])\n",
    "            writer.writerow(fe)\n",
    "    # return with/without\n",
    "    return \"\"\"\n",
    "USING PERIODIC COMMIT\n",
    "LOAD CSV WITH HEADERS FROM 'file:///%s.csv' AS row\n",
    "MERGE (e:%s {_id: row._id, text: row.text})\n",
    "WITH row, e\n",
    "UNWIND split(row.news, ',') AS news_piece\n",
    "MERGE (n:NEWS {_id: news_piece})\n",
    "MERGE (e)-[:liga]-(n);\"\"\" % (csv_name, label), \"\"\"\n",
    "USING PERIODIC COMMIT\n",
    "LOAD CSV WITH HEADERS FROM 'file:///%s.csv' AS row\n",
    "MERGE (e:%s {_id: row._id, text: row.text});\"\"\" % (csv_name, label) # without news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [(\"people\", \"PER\"), (\"orgs\", \"ORG\"), (\"locations\", \"LOC\"), (\"misc\", \"MISC\")]\n",
    "with_news, without_news = \"\", \"\"\n",
    "for filename, label in entities:\n",
    "    w, wo = generate_csv(filename, label)\n",
    "    with_news+=w\n",
    "    without_news+=wo\n",
    "print(\"WITHOUT NEWS:\\n\\n\", without_news)\n",
    "print(\"\\n\", \"-\"*50, \"\\n\")\n",
    "print(\"WITH NEWS:\\n\\n\", with_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../neo4j/import/news.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "    fields = [\"_id\", \"title\"]\n",
    "    writer = csv.DictWriter(out, fieldnames=fields, delimiter=\",\")\n",
    "    writer.writeheader()\n",
    "    for n in db[\"final_news\"].find({}, {k: True for k in fields}, no_cursor_timeout=True):\n",
    "        writer.writerow(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "USING PERIODIC COMMIT\n",
    "LOAD CSV WITH HEADERS FROM 'file:///news.csv' AS row\n",
    "MERGE (n:NEWS {_id: row._id, title: row.title});\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each news piece connecting two entities -> create a relation between those entities \n",
    "# # or\n",
    "# # for each entity, iterate all entites and intersect news, create new relationship with given Weight\n",
    "# def output_connections_with_news():\n",
    "#     with open(\"../neo4j/import/connections_with_news.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "#         writer = csv.DictWriter(out, fieldnames=[\"_id1\", \"_id2\", \"weight\", \"news\"], delimiter=\",\")\n",
    "#         writer.writeheader()\n",
    "#         seen = set()\n",
    "#         pbar = tqdm(total= db[\"final_entities\"].count_documents({}))\n",
    "#         for e1 in db[\"final_entities\"].find({}, {\"news\": True}, no_cursor_timeout=True):\n",
    "#             seen.add(e1[\"_id\"])\n",
    "#             for e2 in db[\"final_entities\"].find({}, {\"news\": True}, no_cursor_timeout=True):\n",
    "#                 if e2[\"_id\"] in seen: continue\n",
    "#                 common_news = list(set(e1[\"news\"]) & set(e2[\"news\"]))\n",
    "#                 if len(common_news):\n",
    "#                     writer.writerow({\"_id1\": e1[\"_id\"], \"_id2\": e2[\"_id\"], \"weight\": len(common_news), \"news\": \",\".join(common_news)})\n",
    "#             pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "USING PERIODIC COMMIT\n",
    "LOAD CSV WITH HEADERS FROM 'file:///connections_with_news.csv' AS row\n",
    "MERGE (e1 {_id: row._id1})\n",
    "MERGE (e2 {_id: row._id2})\n",
    "WITH row, e1, e2\n",
    "MERGE (e1)-[:rel{weight: toInteger(row.weight), news: split(row.news, ',')}]-(e2);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each news piece connecting two entities -> create a relation between those entities \n",
    "# or\n",
    "# for each entity, iterate all entites and intersect news, create new relationship with given Weight\n",
    "def output_connections_without_news(min_common=1):\n",
    "    with open(\"../neo4j/import/connections_%d.csv\" % min_common, \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "        writer = csv.DictWriter(out, fieldnames=[\"_id1\", \"_id2\", \"weight\"], delimiter=\",\")\n",
    "        writer.writeheader()\n",
    "        entities = list(db[\"final_entities\"].find({}, {\"news\": True}, no_cursor_timeout=True)) # loading to RAM\n",
    "        pbar = tqdm(total= len(entities))\n",
    "        for i, e1 in enumerate(entities):\n",
    "            for e2 in entities[i+1:]:\n",
    "                common_news = list(set(e1[\"news\"]) & set(e2[\"news\"]))\n",
    "                if len(common_news)>=min_common:\n",
    "                    writer.writerow({\"_id1\": e1[\"_id\"], \"_id2\": e2[\"_id\"], \"weight\": len(common_news)})\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "USING PERIODIC COMMIT\n",
    "LOAD CSV WITH HEADERS FROM 'file:///connections.csv' AS row\n",
    "MERGE (e1 {_id: row._id1})\n",
    "MERGE (e2 {_id: row._id2})\n",
    "WITH row, e1, e2\n",
    "MERGE (e1)-[:rel{weight: toInteger(row.weight)}]-(e2);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_connections_without_news(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Below is the import code for neo4j-admin import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV entities\n",
    "```\n",
    "_id:ID,text,len:int,:LABEL\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../neo4j/import/i_entities.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "    fields = ['_id', 'text', \"len\", \"label\"]\n",
    "    writer = csv.DictWriter(out, fieldnames=fields, delimiter=\",\")\n",
    "    out.write(\"_id:ID,text,len:int,:LABEL\\n\")\n",
    "    for fe in db[\"final_entities\"].find({}, {k: True for k in fields}, no_cursor_timeout=True):\n",
    "        fe[\"text\"] = fe[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        writer.writerow({\"_id\": fe[\"_id\"], \"text\": fe[\"text\"], \"len\": fe[\"len\"], \"label\": fe[\"label\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV relationships\n",
    "```\n",
    ":START_ID,:END_ID,weight:int,:TYPE\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../neo4j/import/i_connections.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "    writer = csv.DictWriter(out, fieldnames=[\"start_id\", \"end_id\", \"weight\"], delimiter=\",\")\n",
    "    out.write(\":START_ID,:END_ID,weight:int\\n\")\n",
    "    with open(\"../neo4j/import/connections_1.csv\", \"r\", encoding=\"utf-8\") as processed:\n",
    "        reader = csv.reader(processed, delimiter=',')\n",
    "        next(reader) # skip headers\n",
    "        for row in reader:\n",
    "            writer.writerow({\"start_id\": row[0], \"end_id\": row[1], \"weight\": row[2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "FROM cwd = /\n",
    "\n",
    "neo4j-admin import --id-type=STRING --nodes=import/i_entities.csv --relationships=rel=import/i_connections.csv\n",
    "\n",
    "neo4j-admin import --id-type=STRING --nodes=i_entities.csv --relationships=rel=i_connections.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy neo4j\n",
    "* create new local version from newly generated data\n",
    "* copy neo4j/data/databases and neo4j/data/transactions into container volume\n",
    "* restart docker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit1910f3770f18458580fdf7baa626fc0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}