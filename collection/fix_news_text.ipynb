{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "This script will look for all news pieces in `db[\"news\"]` collection that have their `id_title==text` this is probably due to multiple updates over the same news piece where the last one actually ovewrote the previous text, this is further guaranteed because we only look for `processed_entities=True` meaning they had to have a valid text somewhere in the past otherwise they would not have any entity processed associated with them. They must also not have been marked as invalid aka `valid=False`. \n",
    "\n",
    "Parallel processing and the `explore_news_piece` function has been copied from *collect.py*\n",
    "\n",
    "Additionally, after the texts have been fixed, all the entities are iterated and added to the respective news to ensure that, even if they were deleted during the previously described news overwriting process, the news documents will have the final correct value for `entities`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "from src.dbmongo import DbMongo, get_db\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parse_config(\"config.json\")\n",
    "db = get_db(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from newspaper import Article\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logger.catch\n",
    "def explore_news_piece(db, n):\n",
    "    logger.info(\"fetching (%s) %s\" % (n[\"_id\"], n[\"url\"]))\n",
    "    a = Article(n[\"url\"], _language=\"pt\")\n",
    "    html = try_request(n[\"url\"])\n",
    "    if not html:\n",
    "        if html == False:  # resource will never be available\n",
    "            n[\"valid\"] = False\n",
    "            n[\"processed\"] = True\n",
    "            logger.error(\"%s will never be available\" % (n[\"url\"]))\n",
    "            db.upsert_news_piece(n)\n",
    "        return\n",
    "    a.download(input_html=html.text)\n",
    "    try:\n",
    "        a.parse()\n",
    "        text = assert_valid_article(a)\n",
    "        # logger.info(\"%s[%s]\" % (n[\"_id\"], n[\"url\"]))\n",
    "        n[\"text\"] = text\n",
    "        n[\"image\"] = a.top_image\n",
    "    except Exception as e:\n",
    "        logger.error(\"[%s] while parsing %s\" % (e, n))\n",
    "        n[\"valid\"] = False\n",
    "    n[\"processed\"] = True\n",
    "    db.upsert_news_piece(n)\n",
    "    logger.info(\"done %s\" % n[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incomplete(): return db[\"news\"].find_one({\"$where\": \"this.text==this._id_title\", \"processed_entities\": True, \"valid\": {\"$exists\": False}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting...\")\n",
    "batch_limit = 500\n",
    "while incomplete():\n",
    "    print(\"Next loop, incomplete = %s\" % incomplete())\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        pool.map(\n",
    "            lambda n: explore_news_piece(db, n),\n",
    "            db[\"news\"].find({\"$where\": \"this.text==this._id_title\", \"processed_entities\": True, \"valid\": {\"$exists\": False}}).limit(batch_limit)\n",
    "        )\n",
    "print(\"ALL Problematic have been fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percorrer todas as entidades e addToSet nas noticias\n",
    "from pymongo import UpdateOne\n",
    "import pymongo\n",
    "for fe in db[\"entities\"].find({}, no_cursor_timeout=True):\n",
    "    try:\n",
    "        db[\"news\"].bulk_write([UpdateOne({'_id': n}, {\n",
    "            \"$addToSet\": {\"entities.%s\" % fe[\"label\"]: {\"$each\": [{\"_id\": fe[\"_id\"], \"text\": fe[\"text\"]}]}}\n",
    "        }, upsert=True) for n in fe[\"news\"]], ordered=False)\n",
    "    except pymongo.errors.BulkWriteError: pass  # ignore duplicate insertion errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por fim, fazer dump e guardar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit1910f3770f18458580fdf7baa626fc0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}